import numpy as np
from functools import reduce
from sklearn.linear_model import LinearRegression

xs = np.arange(0,250).tolist()
ys_prim = [[0.13080761986505496, 0.13086827360942233, 0.13083851894237417], [0.13084538540400067, 0.13086712919915122, 0.13083699306201274], [0.130910235319362, 0.13085530362635006, 0.13089230622511505], [0.13089230622511505, 0.13084538540400067, 0.1309480008583077], [0.130919772071621, 0.1309529599694824, 0.1309712705338197], [0.13095372290966312, 0.1310162840044823, 0.1309945402093317], [0.1310082731325847, 0.13104298691080754, 0.13098919962806665], [0.13107121569749422, 0.13103459456881958, 0.13105748277424123], [0.13105748277424123, 0.13101437665403048, 0.13105328660324728], [0.1310761748086689, 0.1310929594926448, 0.13104985337243402], [0.13113301385213266, 0.13107464892830747, 0.1310548124836087], [0.13109601125336767, 0.1310410795603557, 0.13112309562978328], [0.13106778246668097, 0.13107731921894, 0.1310937224328255], [0.13105061631261475, 0.13112156974942185, 0.1310849486207472], [0.1310490904322533, 0.1311063109458074, 0.13106091600505448], [0.13106892687695207, 0.13110516653553633, 0.13105137925279547], [0.13111355887752427, 0.1310754118684882, 0.1310937224328255], [0.13103421309872923, 0.13111889945878932, 0.13107770068903035], [0.1310929594926448, 0.13109715566363875, 0.13106015306487376], [0.13113606561285554, 0.1310830412702954, 0.13109410390291587], [0.1310986815440002, 0.13104413132107862, 0.13112843621104833], [0.13106625658631954, 0.13108227833011468, 0.13110364065517488],
[0.13104680161171114, 0.1311314879717712, 0.13107655627875928], [0.13109334096273514, 0.13110592947571706, 0.13104222397062681], [0.13107846362921108, 0.1310986815440002, 0.13104413132107862], [0.1311314879717712, 0.13107083422740387, 0.1310723601077653], [0.13108761891137974, 0.13102925398755455, 0.1311200438690604], [0.13104756455189184, 0.13107312304794602, 0.13107960803948215], [0.1310452757313497, 0.13111699210833752, 0.13106587511622916], [0.1310849486207472, 0.13109830007390982, 0.1310395536799943], [0.13112309562978328, 0.13107083422740387, 0.13108456715065686], [0.13108723744128936, 0.13103039839782563, 0.13112652886059653], [0.13106206041532556, 0.1310742674582171, 0.13106549364613881], [0.13102658369692202, 0.1310937224328255, 0.13105176072288582], [0.13107083422740387, 0.13107846362921108, 0.13101094342321723], [0.1310937224328255, 0.13106587511622916, 0.13106549364613881], [0.13101361371384976, 0.13110135183463273, 0.1310529051331569], [0.1310849486207472, 0.13108838185156046, 0.1310490904322533], [0.13112309562978328, 0.13104832749207257, 0.13107884509930143], [0.13107770068903035, 0.13103268721836778, 0.13111279593734354], [0.13105939012469303, 0.13110402212526526, 0.13106587511622916], [0.13101056195312685, 0.1310296354576449, 0.1310811339198436], [0.13104298691080754, 0.13099988079059677, 0.1310742674582171], [0.13101170636339796, 0.13104642014162077, 0.13105061631261475], [0.13100102520086784, 0.13107731921894, 0.13103383162863888], [0.1310410795603557, 0.13105710130415088, 0.1310178098848437], [0.1310792265693918, 0.13102429487637984, 0.13105824571442196], [0.13102467634647022, 0.13097889993562692, 0.13105519395369908], [0.1309956846196028, 0.13102849104737382, 0.13102315046610877], [0.1310548124836087, 0.13099606608969316, 0.131040316620175], [0.1310391722099039, 0.1309781369954462, 0.1310471830818015], [0.1309975919700546, 0.13102734663710275, 0.13102315046610877], [0.13098996256824738, 0.13106206041532556, 0.13099110697851846], [0.13100216961113892, 0.13096287819183178, 0.13103497603890996], [0.13097661111508477, 0.13101437665403048, 0.13100216961113892], [0.13095715614047637, 0.1310334501585485, 0.1309731778842715], [0.13101895429511481, 0.13099110697851846, 0.13095143408912097], [0.13102696516701237, 0.13096707436282573, 0.1310178098848437], [0.13100789166249435, 0.1309754667048137, 0.13104069809026536], [0.1309743222945426, 0.1310296354576449, 0.13101552106430156], [0.13095906349092817, 0.13104603867153042, 0.1309907255084281], [0.13103383162863888, 0.1310082731325847, 0.1309693631833679], [0.13103421309872923, 0.130980044345898, 0.1310277281071931], [0.1310384092697232, 0.13098233316644017, 0.13103306868845815], [0.1310063657821329, 0.13095334143957277, 0.13104298691080754], [0.13098424051689198, 0.13103306868845815, 0.13101094342321723], [0.13098042581598837, 0.13104565720144004, 0.13097661111508477], [0.13102543928665095, 0.13099377726915098, 0.13096516701237393], [0.1310334501585485, 0.13097279641418116, 0.1309888181579763], [0.13098271463653052, 0.13093998998641013, 0.13101285077366903], [0.13095257849939204, 0.13101170636339796, 0.13098042581598837], [0.130956774670386, 0.13102238752592804, 0.13094761938821733], [0.13100369549150037, 0.1309781369954462, 0.13093998998641013], [0.1309975919700546, 0.1309529599694824, 0.13100903607276543], [0.13096325966192213, 0.13093617528550652, 0.13100331402141002], [0.13095181555921132, 0.13101170636339796, 0.13094189733686193], [0.13101246930357868, 0.13095563026011492, 0.13100293255131965], [0.13096478554228358, 0.13094151586677158, 0.1309949216794221], [0.13093960851631978, 0.13099988079059677, 0.13096516701237393], [0.13093045323415112, 0.13099835491023532, 0.1309411343966812], [0.1309956846196028, 0.1309617337815607, 0.13093655675559687], [0.1309964475597835, 0.13094494909758483, 0.1309964475597835], [0.1309586820208378, 0.13093655675559687, 0.13098157022625945], [0.1309296902939704]]
ys = (ys_prim + ys_prim + ys_prim)[2:]

eg = [[2, 3 ,4], [10, 18, 4], [24, 35, 1]]

reg = LinearRegression(fit_intercept=True)

def unnest(l):
    return map(lambda e: e[0], l)

def elementwise_addition(l):
    zipped_l = zip(l[0], l[1], l[2])
    print(list(zipped_l))
    unnested_zipped_l = unnest(zipped_l)
    print(list(unnested_zipped_l))

# elementwise_addition(eg)
print(len(ys))
reg.fit(np.array(xs).reshape(-1,1), np.array(ys))